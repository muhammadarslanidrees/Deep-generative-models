{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn import preprocessing, metrics\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras import backend as K   # 'generic' backend so code works with either tensorflow or theano\n",
    "from keras.layers import Input, Dense, Lambda, BatchNormalization, Layer\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "np.random.seed(237)\n",
    "\n",
    "#, index_col=0\n",
    "csv = pd.read_csv(r\"\\data\\new_dataset3_train3.csv\", index_col=0)\n",
    "print (csv.shape)\n",
    "\n",
    "data = csv.astype('float32').copy()\n",
    "#data = shuffle(data)\n",
    "\n",
    "\n",
    "csv2 = pd.read_csv(r\"\\data\\new_dataset3_test3.csv\", index_col=0)\n",
    "print (csv2.shape)\n",
    "csv2 = csv2.fillna(0)\n",
    "\n",
    "\n",
    "data2 = csv2.astype('float32').copy()\n",
    "#data2 = shuffle(data2)\n",
    "\n",
    "new_data = data.append(data2)\n",
    "\n",
    "column = new_data.columns.values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "print (new_data.isnull().values.any())\n",
    "\n",
    "new_data.shape\n",
    "\n",
    "data_norm = (data.ix[:, 0:-1] - new_data.ix[:, 0:-1].mean()) / (new_data.ix[:, 0:-1].max() - new_data.ix[:, 0:-1].min())\n",
    "data_df =  pd.concat([data_norm, data.ix[:, -1]], axis=1)\n",
    "\n",
    "#data_df = shuffle(data_df)\n",
    "\n",
    "print (data_df.head(5))\n",
    "\n",
    "data_norm2 = (data2.ix[:, 0:-1] - new_data.ix[:, 0:-1].mean()) / (new_data.ix[:, 0:-1].max() - new_data.ix[:, 0:-1].min())\n",
    "data_df2 =  pd.concat([data_norm2, data2.ix[:, -1]], axis=1)\n",
    "\n",
    "#data_df = shuffle(data_df)\n",
    "\n",
    "print (data_df2.head(5))\n",
    "\n",
    "\n",
    "data_df.to_csv('new_dataset3_train3_normalized.csv', encoding='utf-8', sep=',', index= False)\n",
    "data_df2.to_csv('new_dataset3_test3_normalized.csv', encoding='utf-8', sep=',', index= False)\n",
    "\n",
    "\n",
    "data_norm3 = (new_data.ix[:, 0:-1] - new_data.ix[:, 0:-1].mean()) / (new_data.ix[:, 0:-1].max() - new_data.ix[:, 0:-1].min())\n",
    "data_df3 =  pd.concat([data_norm3, new_data.ix[:, -1]], axis=1)\n",
    "\n",
    "\n",
    "data_df3 = shuffle(data_df3)\n",
    "data_df3.head(5)\n",
    "\n",
    "\n",
    "data_df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data.head(n=5)\n",
    "#normal_df = data_df[data_df.fraudRisk == 0] #save normal_df observations into a separate df\n",
    "fraud_df = data_df3[data_df3.TARGET == 1] #do the same for frauds\n",
    "\n",
    "fraud_df=fraud_df.fillna(0)\n",
    "\n",
    "print (fraud_df.head(10))\n",
    "print (fraud_df.shape)\n",
    "\n",
    "\n",
    "print (fraud_df.isnull().values.any())\n",
    "\n",
    "test_split = 0.2 #portion of data used for testing\n",
    "val_split = 0.1 #portion of training data used for validation\n",
    "\n",
    "first_test = int(fraud_df.shape[0] * (1 - test_split))\n",
    "print (first_test)\n",
    "first_val = int(first_test * (1 - val_split))\n",
    "print (first_val)\n",
    "\n",
    "train = fraud_df.iloc[:first_val]\n",
    "print (train.shape)\n",
    "val = fraud_df.iloc[first_val:first_test]\n",
    "print (val.shape)\n",
    "test = fraud_df\n",
    "print (test.shape)\n",
    "\n",
    "x_train_df, x_val_df, x_test_df = train.iloc[:, :-1], val.iloc[:, :-1], test.iloc[:, :-1]\n",
    "y_train_df, y_val_df, y_test_df = train.iloc[:, -1], val.iloc[:, -1], test.iloc[:, -1]\n",
    "print(x_train_df.shape)\n",
    "print(x_train_df.head(5))\n",
    "\n",
    "\n",
    "x_train, x_val, x_test = x_train_df.values, x_val_df.values, x_test_df.values\n",
    "y_train, y_val, y_test = y_train_df.values, y_val_df.values, y_test_df.values\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "x_train, x_val, x_test = scaler.fit_transform(x_train), scaler.fit_transform(x_val), scaler.fit_transform(x_test)\n",
    "\n",
    "\n",
    "fraud_df.isnull().values.any() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam \n",
    "from keras.optimizers import Adadelta\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adamax\n",
    "from keras.layers import LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "\n",
    "\n",
    "hidden_size = 123 #size of the hidden layer in encoder and decoder\n",
    "hidden_size2 = 60 \n",
    "hidden_size3 = 30\n",
    "hidden_size4 = 15\n",
    "hidden_size5 = 7\n",
    "latent_dim = 2 #number of latent variables to learn\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "x = Input(shape=(input_dim,))\n",
    "\n",
    "t = BatchNormalization()(x)\n",
    "t = Dense(hidden_size, activation='tanh' , name='encoder_hidden5')(t)\n",
    "#t = BatchNormalization()(t)\n",
    "t = Dense(hidden_size2, activation='tanh' , name='encoder_hidden4')(t)\n",
    "t = Dense(hidden_size3, activation='tanh' , name='encoder_hidden3')(t)\n",
    "t = Dense(hidden_size4, activation='tanh' , name='encoder_hidden2')(t)\n",
    "t = Dense(hidden_size5, activation='tanh' , name='encoder_hidden')(t)\n",
    "\n",
    "\n",
    "\n",
    "z_mean = Dense(latent_dim, name='z_mean')(t)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(t)\n",
    "z_mean, z_log_var = KLDivergenceLayer()([z_mean, z_log_var])\n",
    "\n",
    "#, mean=0., stddev=1.\n",
    "#numpy ramdom integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 352)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 352)          1408        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_hidden5 (Dense)         (None, 123)          43419       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_hidden4 (Dense)         (None, 60)           7440        encoder_hidden5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_hidden3 (Dense)         (None, 30)           1830        encoder_hidden4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_hidden2 (Dense)         (None, 15)           465         encoder_hidden3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_hidden (Dense)          (None, 7)            112         encoder_hidden2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            16          encoder_hidden[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            16          encoder_hidden[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "kl_divergence_layer_1 (KLDiverg [(None, 2), (None, 2 0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_sampled (Lambda)              (None, 2)            0           kl_divergence_layer_1[0][0]      \n",
      "                                                                 kl_divergence_layer_1[0][1]      \n",
      "==================================================================================================\n",
      "Total params: 54,706\n",
      "Trainable params: 54,002\n",
      "Non-trainable params: 704\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "decoder_hidden (Dense)       (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "decoder_hidden2 (Dense)      (None, 7)                 21        \n",
      "_________________________________________________________________\n",
      "decoder_hidden3 (Dense)      (None, 15)                120       \n",
      "_________________________________________________________________\n",
      "decoder_hidden4 (Dense)      (None, 30)                480       \n",
      "_________________________________________________________________\n",
      "decoder_hidden5 (Dense)      (None, 60)                1860      \n",
      "_________________________________________________________________\n",
      "decoder_hidden6 (Dense)      (None, 123)               7503      \n",
      "_________________________________________________________________\n",
      "decoded_mean (Dense)         (None, 352)               43648     \n",
      "=================================================================\n",
      "Total params: 53,646\n",
      "Trainable params: 53,642\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              [(None, 2), (None, 2), (N 54706     \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 352)               53646     \n",
      "=================================================================\n",
      "Total params: 108,352\n",
      "Trainable params: 107,644\n",
      "Non-trainable params: 708\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    #batch = K.shape(z_mean)[0] #new\n",
    "    #dim = K.int_shape(z_mean)[1] #new\n",
    "    #epsilon = K.random_normal(shape=(batch, dim)) #new\n",
    "    epsilon = K.random_normal(shape=K.shape(z_mean), mean=0. , stddev= 1. )\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "z = Lambda(sampling, name='z_sampled')([z_mean, z_log_var])\n",
    "#t = BatchNormalization()(z)\n",
    "encoder = Model(x, [z_mean, z_log_var, z])\n",
    "encoder.summary()\n",
    "\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "\n",
    "\n",
    "t = Dense(latent_dim, activation='tanh', name='decoder_hidden')(latent_inputs)\n",
    "t = BatchNormalization()(t)\n",
    "t = Dense(hidden_size5, activation='tanh' , name='decoder_hidden2')(t)\n",
    "t = Dense(hidden_size4, activation='tanh' , name='decoder_hidden3')(t)\n",
    "t = Dense(hidden_size3, activation='tanh' , name='decoder_hidden4')(t)\n",
    "t = Dense(hidden_size2, activation='tanh', name='decoder_hidden5')(t)\n",
    "t = Dense(hidden_size, activation='tanh', name='decoder_hidden6')(t)\n",
    "decoded_mean = Dense(input_dim, activation='tanh', name='decoded_mean')(t)\n",
    "\n",
    "\n",
    "\n",
    "decoder = Model(latent_inputs, decoded_mean)\n",
    "decoder.summary()\n",
    "\n",
    "outputs = decoder(encoder(x)[2])\n",
    "\n",
    "#decoder = Model(latent_inputs, outputs)\n",
    "#decoder.summary()\n",
    "\n",
    "vae = Model(x, outputs)\n",
    "\n",
    "def rec_loss(y_true, y_pred):\n",
    "    return K.sum(K.square(y_true - y_pred), axis=-1)\n",
    "\n",
    "def kl_loss(y_true, y_pred):\n",
    "    return - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "\n",
    "def vae_loss(x, decoded_mean):\n",
    "    rec_loss = K.sum(K.square(x - decoded_mean), axis=-1)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return K.mean((rec_loss + kl_loss) / 2)\n",
    "\n",
    "vae.compile(optimizer=Adam(lr=1e-2), loss=vae_loss, metrics=[rec_loss, kl_loss])\n",
    "vae.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, min_delta=1e-5) #stop training if loss does not decrease with at least 0.00001\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', patience=5, min_delta=1e-5, factor=0.2) #reduce learning rate (divide it by 5 = multiply it by 0.2) if loss does not decrease with at least 0.00001\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "#collect training data in history object\n",
    "history = vae.fit(x_train, x_train, \n",
    "                  validation_data=(x_val, x_val), \n",
    "                  batch_size=batch_size, epochs=n_epochs, \n",
    "                  callbacks=callbacks)\n",
    "\n",
    "\n",
    "\n",
    "z_mean, z_log_var, z = encoder.predict(x_test,batch_size=128)\n",
    "\n",
    "#for i, yi in enumerate(encode_set):\n",
    " #   print (yi)\n",
    "\n",
    "#decoder_set = decoder.predict(encode_set, batch_size=128)\n",
    "\n",
    "#print (decoder_set)\n",
    "\n",
    "#encode_set.shape\n",
    "print (z)\n",
    "\n",
    "\n",
    "\n",
    "print (z.shape)\n",
    "print (z_mean.shape)\n",
    "print (z_log_var.shape)\n",
    "\n",
    "\n",
    "decoded_sam = decoder.predict(z)\n",
    "print (decoded_sam)\n",
    "print (decoded_sam.shape)\n",
    "\n",
    "\n",
    "decoded_sam2 = decoder.predict(z_mean)\n",
    "print (decoded_sam2)\n",
    "print (decoded_sam2.shape)\n",
    "\n",
    "decoded_sam3 = decoder.predict(z_log_var)\n",
    "print (decoded_sam3)\n",
    "print (decoded_sam3.shape)\n",
    "\n",
    "z_mean = shuffle(z_mean)\n",
    "decoded_sam4 = decoder.predict(z_mean)\n",
    "print (decoded_sam4)\n",
    "print (decoded_sam4.shape)\n",
    "\n",
    "z = shuffle(z)\n",
    "decoded_sam5 = decoder.predict(z)\n",
    "print (decoded_sam5)\n",
    "print (decoded_sam5.shape)\n",
    "\n",
    "arr = np.array(decoded_sam)\n",
    "arr2 = np.array(decoded_sam2)\n",
    "arr3 = np.array(decoded_sam3)\n",
    "arr4 = np.array(decoded_sam4)\n",
    "arr5 = np.array(decoded_sam5)\n",
    "\n",
    "#arr = np.array(arr)\n",
    "#samples=[]\n",
    "columnsTitles = column\n",
    "\n",
    "#arr = transpose(arr)\n",
    "#arr.shape\n",
    "print (arr)\n",
    "\n",
    "\n",
    "columnsTls= columnsTitles[:-1]\n",
    "#print (columnsTls)\n",
    "\n",
    "df = pd.DataFrame(arr, columns=columnsTls )\n",
    "print (df.head(10))\n",
    "df2 = pd.DataFrame(arr2, columns=columnsTls )\n",
    "print (df2.head(10))\n",
    "df3 = pd.DataFrame(arr3, columns=columnsTls )\n",
    "print (df3.head(10))\n",
    "df4 = pd.DataFrame(arr4, columns=columnsTls )\n",
    "print (df4.head(10))\n",
    "df5 = pd.DataFrame(arr5, columns=columnsTls )\n",
    "print (df5.head(10))\n",
    "df = df.append(df2)\n",
    "df = df.append(df3)\n",
    "df = df.append(df4)\n",
    "df = df.append(df5)\n",
    "\n",
    "df = shuffle(df)\n",
    "print (df.head(10))\n",
    "print (df.shape)\n",
    "df['TARGET'] = 1.0\n",
    "print (df.head(10))\n",
    "print (df.shape)\n",
    "\n",
    "#norm = pd.read_csv(\"train(new)2_normalize_new2.csv\")\n",
    "\n",
    "#normt = norm.astype('float32').copy()\n",
    "#train_data = shuffle(normt)\n",
    "#print (train_data.head(10))\n",
    "\n",
    "print(data_df3.TARGET.value_counts())\n",
    "column = data_df3.columns.values.tolist()\n",
    "\n",
    "\n",
    "df = df.reindex(columns=columnsTitles)\n",
    "data_df3 =data_df3.append(df)\n",
    "#train_data = shuffle(train_data)\n",
    "print (data_df3.head(10))\n",
    "print (data_df3.shape)\n",
    "\n",
    "print(data_df.TARGET.value_counts())\n",
    "\n",
    "data_df.to_csv(r\"\\data\\new_dataset3_train3_normalized_balance_new.csv\", encoding='utf-8', sep=',', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
